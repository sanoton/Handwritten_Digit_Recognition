{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem C\n",
    "Build a deep convolutional neural network (CNN) that achieves test accuracy of at least 0.985 on MNIST. \n",
    "Use this notebook to write your code for CNN by filling in the sections marked `# TODO` and running all cells. You are allowed to change the other parts of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset\n",
    "First we need to download Pytorch and import dependencies. In Pytorch, you need to create a `Dataset` first and then use a `DataLoader` to iterate over the data samples. We can directly download MNIST using `torchvision.datasets` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size, which is how many samples you use for training in one iteration\n",
    "batch_size_train = 32\n",
    "batch_size_test = 1024\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "# MNIST datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Sample \n",
    "We can check the dataloader and visualize one sample of the hand-written digit image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(4)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my code\n",
    "example_datas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample = example_datas[1022][0]\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "Now we can start to build our CNN model.  You are only allowed to use \n",
    "- Linear Layers\n",
    "- Conv2D\n",
    "- MaxPool2D\n",
    "- BatchNorm2D\n",
    "- Dropout Layers\n",
    "- ReLU and Softmax\n",
    "and have fewer than 1million parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"This class defines your deep learning model that extends a Module class\n",
    "      The constructor of your class defines the layers of the model. \n",
    "      The forward() function defines how to forward propagate \n",
    "      input through the defined layers of the model.\n",
    "      Many layers are available, such as Linear for fully connected layers, \n",
    "      Conv2d for convolutional layers, and MaxPool2d for pooling layers.\n",
    "\n",
    "    \"\"\"\n",
    "    #============================================\n",
    "    # TODO: Implement CNN model.\n",
    "    #=============================================\n",
    "    #A picture shape: 28 x 28 x 1  and there are 1024 pictures --> 28 * 28 * 1\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)   # output_shape=(24,24,16)     (28-5+2(0))/1 + 1 = 24\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)     # output_shape=(12,12,30)\n",
    "        #self.bnm1 = nn.BatchNorm2d(16)\n",
    "        #self.drop_layer = nn.Dropout(p=0.2)         # To avoid the training model overfitting \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=40, kernel_size=5)  # output_shape=(8,8,40)\n",
    "        #self.bnm2 = nn.BatchNorm2d(40)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)     # output_shape=(4,4,40)\n",
    "        self.fc1 = nn.Linear(4*4*40, 10)           # Applies a linear transformation to the incoming data: y = x*W^T + b\n",
    "                                                   # 10 possible classes: 0 to 9 \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #x = self.bnm1(x)   # To avoid the vanishing/exploding gradient\n",
    "        #x = self.drop_layer(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = self.bnm2(x)\n",
    "        x = x.view(-1, 4*4*40)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22866"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "sum(p.numel() for p in cnn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    iterator = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(iterator):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        iterator.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e770260a074826a937751f1bb73929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0789, Accuracy: 9774/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7795728155384d7e9a2d181fbece43b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0653, Accuracy: 9829/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb05ac6a8e4041f9b034fe4fa04b3a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9852/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e9b6ad34f34afd9e5405b9f5f8391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9866/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c9d307ca474cf4bcc019083c1c7121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9845/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22490c1609894cc5a88608053582f329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0473, Accuracy: 9865/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dd300193bf40859ec075b3d3f15d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0500, Accuracy: 9853/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf0c2fcf64f45b5b4d9d39bdbd7ac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0564, Accuracy: 9838/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db44c576a1845799d233ed9de8d94ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0555, Accuracy: 9825/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b1a41156694d278a75b4b785f045fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1875), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test set: Average loss: 0.0450, Accuracy: 9873/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "model = CNN().to(device) #using cpu here\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=0.01) #L2 regularization weight_decay = lamda\n",
    "num_epoch = 10\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
